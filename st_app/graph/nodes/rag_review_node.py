# st_app/graph/nodes/rag_review_node.py
from __future__ import annotations
from typing import Dict, Any, List, Optional, Tuple
import os

from langchain.schema import Document  # ë¬¸ì„œ íƒ€ì… íŒíŠ¸ìš©

# ê³µìš© ë ˆì´ì–´
from st_app.rag.embedder import load_faiss_index
from st_app.rag.prompt import get_rag_review_prompt

# ìƒíƒœ/í—¬í¼
from st_app.utils.state import State
from st_app.rag.llm import get_upstage_llm

# --------- ëª¨ë“ˆ ì „ì—­ ìºì‹œ ---------
_VS = None         # FAISS vector store


def _faiss_dir() -> str:
    """
    ì¸ë±ìŠ¤ ê²½ë¡œ: í™˜ê²½ë³€ìˆ˜ RAG_FAISS_DIR ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
    """
    return os.getenv("RAG_FAISS_DIR", "st_app/db/faiss_index")


def _ensure_vs():
    """
    FAISS index ë¡œë”©(1íšŒ)
    """
    global _VS
    if _VS is None:
        _VS = load_faiss_index(_faiss_dir())
        if _VS is None:
            raise RuntimeError("FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return _VS


def _short_src(md: Dict[str, Any]) -> str:
    """
    ê·¼ê±° í‘œê¸°ì— ë“¤ì–´ê°ˆ ê°„ëµ ì†ŒìŠ¤ ë¬¸ìì—´ ìƒì„±
    """
    try:
        rating = md.get("rating")
        date = md.get("date", "") or ""
        platform = md.get("platform", "unknown") or "unknown"
        rate_s = f"|rating={rating}" if rating is not None else ""
        platform_s = f"|{platform}" if platform and platform != "unknown" else ""
        return f"review|{date}{rate_s}{platform_s}"
    except Exception as e:
        print(f"Error in _short_src: {e}, metadata: {md}")
        return "review|unknown"


def _format_context(docs_with_scores: List[Tuple[Document, float]]) -> str:
    """
    ëª¨ë¸ì— ì œê³µí•  ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ ìƒì„±
    - ê° ì²­í¬ì™€ ë©”íƒ€ë¥¼ í•¨ê»˜ ì „ë‹¬ (ëª¨ë¸ì´ ì¶œë ¥ì—ì„œ ê·¼ê±° ì¸ìš©ì„ êµ¬ì„±í•˜ê¸° ì‰¬ì›€)
    - ìœ ì‚¬ë„ ì ìˆ˜ë„ í¬í•¨
    """
    if not docs_with_scores:
        return ""
    parts = []
    for i, (doc, score) in enumerate(docs_with_scores):
        parts.append(
            f"[Review {i+1}]\n"
            f"{doc.page_content}\n"
            f"(source: {_short_src(doc.metadata)}, similarity: {score:.3f})"
        )
    return "\n\n".join(parts)


def _to_document_hits(docs_with_scores: List[Tuple[Document, float]]) -> List[Dict[str, Any]]:
    """
    ìƒíƒœì— ì €ì¥í•  RAG ê²°ê³¼(ì§„ë‹¨/ì¶œì²˜ìš©)
    """
    results: List[Dict[str, Any]] = []
    for doc, score in docs_with_scores:
        try:
            md = doc.metadata or {}
            results.append({
                "chunk": doc.page_content,
                "date": md.get("date", ""),
                "rating": md.get("rating"),
                "platform": md.get("platform", "unknown"),
                "place": md.get("place", "ë¡¯ë°ì›”ë“œ"),
                "source_row": md.get("source_row"),
                "chunk_index": md.get("chunk_index"),
                "score": float(score)  # ìœ ì‚¬ë„ ì ìˆ˜ ì¶”ê°€
            })
        except Exception as e:
            print(f"Error processing document hit: {e}, metadata: {doc.metadata}")
            # ì—ëŸ¬ê°€ ìˆì–´ë„ ê¸°ë³¸ ì •ë³´ëŠ” í¬í•¨
            results.append({
                "chunk": doc.page_content,
                "date": "",
                "rating": None,
                "platform": "unknown",
                "place": "ë¡¯ë°ì›”ë“œ",
                "source_row": None,
                "chunk_index": None,
                "score": float(score)
            })
    return results


def _filter_by_threshold(docs_with_scores: List[Tuple[Document, float]], threshold: float = 0.6) -> List[Tuple[Document, float]]:
    """
    ìœ ì‚¬ë„ ì„ê³„ê°’ìœ¼ë¡œ í•„í„°ë§
    """
    return [(doc, score) for doc, score in docs_with_scores if score >= threshold]


def rag_review_node(state: State) -> State:
    """
    FAISS ê¸°ë°˜ ë¦¬ë·° RAG ì‘ë‹µ ë…¸ë“œ (ì»¤ìŠ¤í…€ FAISS ì‚¬ìš©)
    ì…ë ¥:
      - state.user_input
      - state.conversation_history
    ì¶œë ¥:
      - state.result (ì‘ë‹µ)
      - state.retrieved_reviews (ê²€ìƒ‰ëœ ë¦¬ë·°ë“¤)
      - state.review_query, state.current_node="rag_review"
    ì—ëŸ¬:
      - state.error ì— ê°„ë‹¨í•œ ì‚¬ìœ  ì €ì¥
    """
    try:
        # 1) ì¿¼ë¦¬ í™•ë³´
        question = state.get("user_input", "").strip()
        if not question:
            state["result"] = "ì§ˆë¬¸ì´ ë¹„ì–´ ìˆì–´ìš”. ì–´ë–¤ ì ì´ ê¶ê¸ˆí•œê°€ìš”?"
            state["current_node"] = "rag_review"
            return state

        state["review_query"] = question

        # 2) FAISS ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„
        vs = _ensure_vs()

        # 3) ê²€ìƒ‰ ìˆ˜í–‰ - similarity_search_with_score ì‚¬ìš©í•˜ì—¬ ì ìˆ˜ë„ í•¨ê»˜ ê°€ì ¸ì˜¤ê¸°
        docs_with_scores: List[Tuple[Document, float]] = vs.similarity_search_with_score(question, k=10)
        
        if not docs_with_scores:
            state["result"] = "ê´€ë ¨ëœ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì‹œê² ì–´ìš”?"
            state["current_node"] = "rag_review"
            state["retrieved_reviews"] = []
            return state

        # 4) ìœ ì‚¬ë„ ì„ê³„ê°’ìœ¼ë¡œ í•„í„°ë§ (ì„ íƒì‚¬í•­)
        # ë„ˆë¬´ ê´€ë ¨ì„±ì´ ë‚®ì€ ë¬¸ì„œëŠ” ì œì™¸
        filtered_docs = _filter_by_threshold(docs_with_scores, threshold=0.4)
        
        # í•„í„°ë§ í›„ì—ë„ ìµœì†Œ 3ê°œëŠ” ìœ ì§€
        if len(filtered_docs) < 3 and len(docs_with_scores) >= 3:
            filtered_docs = docs_with_scores[:3]
        elif not filtered_docs and docs_with_scores:
            filtered_docs = docs_with_scores[:1]  # ìµœì†Œ 1ê°œëŠ” ìœ ì§€
        
        # ìµœì¢…ì ìœ¼ë¡œ ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
        final_docs = filtered_docs[:5]

        # 5) ì»¨í…ìŠ¤íŠ¸/ê·¼ê±° ë©”íƒ€ êµ¬ì„±
        context = _format_context(final_docs)
        state["retrieved_reviews"] = _to_document_hits(final_docs)
        state["rag_context"] = context

        # 6) ê²€ìƒ‰ í’ˆì§ˆ ì •ë³´ ì¶”ê°€
        avg_score = sum(score for _, score in final_docs) / len(final_docs)
        max_score = max(score for _, score in final_docs)
        min_score = min(score for _, score in final_docs)
        state["search_quality"] = {
            "total_found": len(docs_with_scores),
            "filtered_count": len(filtered_docs),
            "used_count": len(final_docs),
            "avg_similarity": avg_score,
            "max_similarity": max_score,
            "min_similarity": min_score
        }

        # 7) í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM í˜¸ì¶œ
        llm = get_upstage_llm(temperature=0.2)
        prompt_text = get_rag_review_prompt(context=context, question=question)
        result = llm.invoke(prompt_text)
        answer: str = result.content

        # 8) ê²€ìƒ‰ í’ˆì§ˆì— ë”°ë¥¸ ì‹ ë¢°ë„ í‘œì‹œ ì¶”ê°€ (ì„ íƒì‚¬í•­)
        confidence_note = ""
        if avg_score < 0.5:
            confidence_note = "\n\nğŸ’¡ *ê²€ìƒ‰ëœ ë¦¬ë·°ì™€ì˜ ê´€ë ¨ì„±ì´ ë‹¤ì†Œ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ë³´ì‹œê² ì–´ìš”?*"
        elif avg_score > 0.7:
            confidence_note = "\n\nâœ¨ *ë§¤ìš° ê´€ë ¨ì„±ì´ ë†’ì€ ë¦¬ë·°ë“¤ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!*"

        # 9) ì‘ë‹µ ì €ì¥ ë° í˜„ì¬ ë…¸ë“œ í‘œì‹œ
        state["result"] = answer + confidence_note
        state["current_node"] = "rag_review"
        state["error"] = None
        
        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        conversation_history = state.get("conversation_history", [])
        conversation_history.append({
            "user": question,
            "assistant": answer
        })
        state["conversation_history"] = conversation_history
        
        return state

    except Exception as e:
        # ì—ëŸ¬ì‹œ ì‚¬ìš©ìì—ê²Œë„ ì§§ê²Œ ì•ˆë‚´í•˜ê³ , ì—ëŸ¬ ì €ì¥
        err_msg = f"ë¦¬ë·° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}"
        state["result"] = err_msg
        state["error"] = str(e)
        state["current_node"] = "rag_review"
        
        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        conversation_history = state.get("conversation_history", [])
        conversation_history.append({
            "user": state.get("user_input", ""),
            "assistant": err_msg
        })
        state["conversation_history"] = conversation_history
        
        return state